{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pixels2bytes/SHALA/blob/add-to-core/core.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXoXxK9WhdqK"
      },
      "source": [
        "# SHALA (Supportive Help Agent and Lifeline Assistant)\n",
        "This is a Python-based chatbot project powered by the Gemini AI Agent, designed to assist individuals experiencing depression by providing ongoing emotional check-ins and real-time crisis intervention. Mental health support systems must be developed and used with sensitivity, responsibility, and respect for the lives they aim to protect. SHALA is not a replacement for certified therapists or mental health professionals. This project is intended for research, prototype development, and educational purposes. **During development, all outgoing call features were pointed to a non-functioning test number.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lum9eIUKdQo8"
      },
      "source": [
        "## Setup Imports, Documents, and Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6FVzNFxxKgK"
      },
      "source": [
        "Install packages and remove conflicting packages from base environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyUsJPNgw4h6",
        "outputId": "6c661a10-72d7-4682-f0fb-52597075724e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping kfp as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping jupyterlab as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping libpysal as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping thinc as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping spacy as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping fastai as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping ydata-profiling as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping google-cloud-bigquery as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping google-generativeai as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip uninstall -qqy kfp jupyterlab libpysal thinc spacy fastai ydata-profiling google-cloud-bigquery google-generativeai\n",
        "!pip install -qU 'langgraph==0.3.21' 'langchain-google-genai==2.1.2' 'langgraph-prebuilt==0.1.7'\n",
        "# !pip install -qU 'whatsapp-python' 'hatch' [Depreciated]\n",
        "!pip install -qU 'gradio_client'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hW063mXhxX2Z"
      },
      "source": [
        "Intialize Imports and set variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "-s-3nMNB0Sog"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import kagglehub\n",
        "import tensorflow as tf\n",
        "import gradio as gr\n",
        "import time\n",
        "from google.colab import drive\n",
        "from google.colab import userdata\n",
        "from typing import Annotated\n",
        "from typing_extensions import TypedDict\n",
        "from langgraph.graph.message import add_messages\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.schema import AIMessage, HumanMessage\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from typing import Literal\n",
        "from IPython.display import Image, display\n",
        "from gradio_client import Client\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "#drive.mount('/content/drive/') # Can't access your drive\n",
        "google_api_key = userdata.get('GOOGLE_API_KEY') # retrieve Google API Key\n",
        "# whatsapp_api_key = userdata.get('WHATSAPP_API_KEY') # retrieve Google API Key\n",
        "provider_num = userdata.get('DUMMY_NUMBER') # Retrieve DUMMY NUMBER. NEVER use a HELP Provider number\n",
        "kaggle_user = userdata.get('KAGGLE_USER') # Retrieve Kaggle dataset download username found in kaggle.json\n",
        "kaggle_download_api = userdata.get('KAGGLE_API') # Retrieve Kaggle dataset download token found in kaggle.json\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = google_api_key\n",
        "ai_agent_model = userdata.get('AGENT_MODEL') # Retrieves Agent AI model\n",
        "agent_model = ChatGoogleGenerativeAI(model=ai_agent_model)\n",
        "user_name = \"Friend\" # Default username. Will be replaced by user's actually name if told\n",
        "emotional = False # If the user is in an emotional state (True). Default False\n",
        "checkin_timer = 86400/2 # Check in with user that has not been active for half a day\n",
        "realtime_test = True # Uploads Gradio Interface for real-world user testing. False runs it in developer mode. Default True.\n",
        "verbose = True\n",
        "\n",
        "checkpoint = \"HuggingFaceTB/SmolLM2-135M-Instruct\" # checkpoint of local model\n",
        "device = \"cpu\"  # \"cuda\" or \"cpu\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "local_model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWQwRbga9cFE"
      },
      "source": [
        "Make root folder and necessary folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4kGpo8p8-Bc",
        "outputId": "3c46daf0-cb32-4e0a-d1ab-59d1a6cba2de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/shala\n"
          ]
        }
      ],
      "source": [
        "# Set project root folder\n",
        "project_root = \"/content/shala\"\n",
        "\n",
        "# Make subfolders\n",
        "os.makedirs(f\"{project_root}/datasets\", exist_ok=True)\n",
        "os.makedirs(f\"{project_root}/repos\", exist_ok=True)\n",
        "os.makedirs(f\"{project_root}/responses\", exist_ok=True)\n",
        "os.makedirs(f\"{project_root}/history\", exist_ok=True)\n",
        "%cd {project_root}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T23TUQE_ATCx"
      },
      "source": [
        "FOR ME TO DELETE ////////////////////////////////////"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "w5LcJHWM4kn5"
      },
      "outputs": [],
      "source": [
        "# !rm -rf shala/cloned repos # trying to delete manually to test but dont have permissions?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5O-_FEYmcYh"
      },
      "source": [
        "### Installation\n",
        "Clone repos for AI Agent and Whatsapp API Templates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbGd6lQkmgpy",
        "outputId": "12500788-7e46-43a1-cc01-84c37c17988c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'repos/agent-starter-pack' already exists and is not an empty directory.\n",
            "fatal: destination path 'repos/whatsapp' already exists and is not an empty directory.\n",
            "fatal: destination path 'repos/openai-whatsapp-chatbot' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/GoogleCloudPlatform/agent-starter-pack.git repos/agent-starter-pack # Clone Agent Starter Pack from Google\n",
        "!git clone https://github.com/filipporomani/whatsapp.git repos/whatsapp # Clone WhatsApp API\n",
        "!git clone https://github.com/simonsanvil/openai-whatsapp-chatbot.git repos/openai-whatsapp-chatbot # Clone Whatsapp API chatbot setup template"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxZRIpiqr2VT"
      },
      "source": [
        "### Download datasets (Depreciated / May Fix Later)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8Ns44rY2v3u"
      },
      "source": [
        "Grab Kaggle Datasets by going to kaggle.com > Settings > API and generate a token, From there add the json file to the project or add the username and key to your secrets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "lkoEBCNT3SGD"
      },
      "outputs": [],
      "source": [
        "kaggle_user = userdata.get('KAGGLE_USER') # Retrieve Kaggle dataset download username found in kaggle.json\n",
        "kaggle_download_api = userdata.get('KAGGLE_API') # Retrieve Kaggle dataset download token found in kaggle.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9rJrM9B3U8q"
      },
      "source": [
        "Automatically downloads the Kaggle datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "YZFRjtRFr7JB"
      },
      "outputs": [],
      "source": [
        "# !kaggle datasets download -d rvarun11/suicidal-ideation-reddit-dataset -p datasets # Suicidal Ideation Reddit\n",
        "# !kaggle datasets download -d natalialech/suicidal-ideation-on-twitter -p datasets # Suicidal Ideation Twitter"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Manual Developer Test"
      ],
      "metadata": {
        "id": "ZAtfbmW9c23D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# user_input = input(\"Testing is the best thing =)\")"
      ],
      "metadata": {
        "id": "XcUrjapecxWl"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7Kz-yBOKKee"
      },
      "source": [
        "### Setup Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeGjVfWsR9Wb"
      },
      "source": [
        "Define Core Instructions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "BmbUbef2SCkq"
      },
      "outputs": [],
      "source": [
        "class ListeningState(TypedDict):\n",
        "    \"\"\"State representing the user's conversations.\"\"\"\n",
        "\n",
        "    # This preserves the conversation history between nodes. The `add_messages` annotation indicates to LangGraph that state is updated by appending returned messages, not replacing them.\n",
        "    message_history: Annotated[list, add_messages]\n",
        "\n",
        "    # UI messages\n",
        "    chat_display: list[list[str]]\n",
        "\n",
        "    # Flag indicating that the recent messages contained risk language\n",
        "    emotional: bool\n",
        "\n",
        "    # Flag indicating that the message is marked as suicidal\n",
        "    alert: bool\n",
        "\n",
        "\n",
        "# The system instruction defines how the chatbot is expected to behave and includes rules for when to call different functions, as well as rules for the conversation, such as tone and what is permitted for discussion.\n",
        "SHALABOT_SYSINT = (\n",
        "    \"system\",\n",
        "    \"You are SHALA, the Supportive Help Agent and Lifeline Assistant, a compassionate mental health check-in chatbot.\"\n",
        "    \"Your sole purpose is to provide emotional support and guide users toward safe actions in moments of distress.\"\n",
        "    \"You are not a licensed therapist or emergency responder. You must never claim to be a substitute for professional medical or psychological help. \"\n",
        "    \"\\n\\n\"\n",
        "    \"**Conversation Guidelines:**\\n\"\n",
        "    \"- Maintain a calm, non-judgmental, empathetic tone at all times.\\n\"\n",
        "    \"- Ask open-ended questions and encourage emotional expression (e.g., 'Can you tell me more about how you're feeling?').\\n\"\n",
        "    \"- Avoid giving advice that could be interpreted as medical, therapeutic, or diagnostic.\\n\"\n",
        "    \"- Never tell a user what actions to take regarding medication, self-harm, or major life decisions.\\n\"\n",
        "    \"- If a user expresses intent to harm themselves or others, respond with pre-scripted messages encouraging them to reach out to real professionals.\\n\"\n",
        "    \"- If suicidal ideation is detected, call `handle_crisis_protocol()` and do not continue casual conversation until resolved.\\n\"\n",
        "    \"\\n\"\n",
        "    \"**Function Use Rules:**\\n\"\n",
        "    \"- Use `handle_crisis_protocol()` if a user mentions self-harm, suicide, or harming others.\\n\"\n",
        "    \"- Use `log_emotional_checkin()` after each emotional status conversation.\\n\"\n",
        "    \"- Use `initiate_reg_checkin()` after each emotional status conversation.\\n\"\n",
        "    \"- Use `suggest_selfcare_options()` only if user is in a non-crisis emotional state.\\n\"\n",
        "    \"- Use `provide_resources(region)` if a user asks for help locating therapists, hotlines, or crisis centers.\\n\"\n",
        "    \"\\n\"\n",
        "    \"**Safety Restrictions:**\\n\"\n",
        "    \"- Never give out or generate real phone numbers or emergency services unless the function `provide_resources()` is available.\\n\"\n",
        "    \"- Treat every instance with seriousness. Do not assume any attempt is made to prank, abuse, or misuse mental health resources.\\n\"\n",
        "    \"\\n\"\n",
        "    \"**Legal Disclaimer:**\\n\"\n",
        "    \"Always remind the user that SHALA is an experimental support assistant and not a replacement for certified help. Use of this bot constitutes agreement to these terms. \"\n",
        "    \"Any misuse of this bot to simulate or falsely trigger emergency resources is a violation of its intended purpose.\"\n",
        ")\n",
        "\n",
        "\n",
        "HELLO_MSG = \"Hi Friend, my name is Shala, what's yours?\" # This is the message the system opens with in meeting the user for the very first time\n",
        "CHECKIN_MSG = \"Hello. How are you doing?\" # This is the message the system opens the conversation with when it has not heard from the user in X amount of time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ss8EXFuF1h4P"
      },
      "source": [
        "Create chatbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "m1g4MLLHAcZ9",
        "outputId": "73c03af2-40b2-44cf-bd56-c1e5b09b1bdf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7e49b4d21e90>"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ],
      "source": [
        "# Initialize state\n",
        "state = ListeningState()\n",
        "\n",
        "def human_node(state: ListeningState=ListeningState(), user_input:str=\"\", realtime_test:bool=False, verbose:bool=False) -> ListeningState:\n",
        "    \"\"\"Display the last model message to the user, and receive the user's input.\"\"\"\n",
        "\n",
        "    message_history = state.get(\"message_history\", [])\n",
        "\n",
        "    if not message_history:\n",
        "      last_msg = \"\"\n",
        "    else:\n",
        "      last_msg = state[\"message_history\"][-1]\n",
        "\n",
        "    if realtime_test:\n",
        "        print(f\"Current State: {state}\")\n",
        "        return state | {\"chat_display\": [(\"user\", user_input)]}\n",
        "\n",
        "    if verbose:\n",
        "      print(\"SHALA:\", last_msg.content)\n",
        "\n",
        "    # If the user is trying to stop talking, go to rest mode\n",
        "    if user_input in {\"q\", \"quit\", \"exit\", \"goodbye\", \"ttyl\", \"cya\"}:\n",
        "        #state[\"rest\"] = True\n",
        "        if state[\"emotional\"] == True: # If they had an emotional rant or experience\n",
        "          set_checkin_timer = 600 # Check on them in 10 minutes instead of half a day\n",
        "\n",
        "    return state | {\"chat_display\": [(\"user\", user_input)]}\n",
        "\n",
        "\n",
        "def update_memory_node(model=agent_model, state: ListeningState=ListeningState()):\n",
        "    \"\"\"Shortens token usage for agent model history\"\"\"\n",
        "\n",
        "    history = state[\"message_history\"]\n",
        "\n",
        "    # To shorten token usage\n",
        "    history_text = \"\"\n",
        "    for m in history:\n",
        "        role = \"User\" if m.type == \"human\" else \"SHALA\"\n",
        "        history_text += f\"{role}: {m.content}\\n\"\n",
        "\n",
        "    # Prompt the LLM to summarize the conversation for agent model\n",
        "    old_history = f\"Summarize this conversation briefly as possible while keeping important context as if you are a therapist taking notes (Notes such as psychological triggers, attitude, progress, mental state, what calms them):\\n\\n{history_text}\"\n",
        "    summary_response = agent_model.invoke(old_history)\n",
        "\n",
        "    return summary_response.content\n",
        "\n",
        "\n",
        "def chatbot(user_input:str=\"\", message_history:list=[], state: ListeningState=ListeningState()) -> ListeningState:\n",
        "    \"\"\"The chatbot itself. A wrapper around the model's own chat interface.\"\"\"\n",
        "\n",
        "    if not message_history:\n",
        "       # If there are no messages, start with the initial hello message\n",
        "      chatbot_msg = AIMessage(content=HELLO_MSG)\n",
        "    else:\n",
        "      # Check if message history is long\n",
        "      if len(message_history) > 50:\n",
        "        longterm_memory = message_history # WILL SETUP LATER\n",
        "        summary = update_memory_node(agent_model, state)\n",
        "        new_history = [summary]\n",
        "        message_history = new_history\n",
        "\n",
        "      # If there are messages, continue the conversation with the model\n",
        "      current_message = [SHALABOT_SYSINT] + message_history + [user_input]\n",
        "      chatbot_msg = agent_model.invoke(current_message)\n",
        "\n",
        "    # Update chat_display (for Gradio UI)\n",
        "    update_display = state.get(\"chat_display\", [])\n",
        "    update_display.append([\"User\", user_input])\n",
        "    update_display.append([\"SHALA\", chatbot_msg])\n",
        "\n",
        "    return state | {\n",
        "        \"message_history\": message_history,    # Update memory for LLM\n",
        "        \"chat_display\": update_display,   # Update Gradio conversation\n",
        "      }, chatbot_msg\n",
        "\n",
        "\n",
        "# Start building a new graph.\n",
        "graph_builder = StateGraph(ListeningState)\n",
        "\n",
        "# Add the chatbot and human nodes to the app graph.\n",
        "graph_builder.add_node(\"SHALA\", chatbot)\n",
        "graph_builder.add_node(\"human\", human_node)\n",
        "\n",
        "# Start with the chatbot again.\n",
        "graph_builder.add_edge(START, \"SHALA\")\n",
        "\n",
        "# The chatbot will always go to the human next.\n",
        "graph_builder.add_edge(\"SHALA\", \"human\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCxW6c1DajYZ"
      },
      "source": [
        "Exit chat capability for user"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "AhJvLxahV8nr"
      },
      "outputs": [],
      "source": [
        "def exit_human_node(state: ListeningState, verbose:bool = False) -> Literal[\"SHALA\", \"__end__\"]:\n",
        "    \"\"\"Route to the chatbot, unless it looks like the user is exiting.\"\"\"\n",
        "\n",
        "    if state.get(\"finished\", False):\n",
        "      if verbose:\n",
        "        print(\"Chat ending\")\n",
        "        return END\n",
        "    else:\n",
        "        return \"SHALA\"\n",
        "\n",
        "\n",
        "graph_builder.add_conditional_edges(\"human\", exit_human_node)\n",
        "\n",
        "chat_with_human_graph = graph_builder.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "User Interface"
      ],
      "metadata": {
        "id": "kzIWzkKikCf4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Special Note: Block for transition to Python script"
      ],
      "metadata": {
        "id": "z9Atqf3BqhNJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def duration_timer(user_input:str, checkin_timer:int, verbose:bool=False):\n",
        "    last_input_time = time.time()  # last user input timestamp\n",
        "\n",
        "    while True:\n",
        "        curr_time = time.time()\n",
        "        elapsed = curr_time - last_input_time\n",
        "        days = int(elapsed // 86400)\n",
        "        hours = int((elapsed % 86400) // 3600)\n",
        "        if verbose:\n",
        "            print(f\"Time since last input: {elapsed:.2f} seconds -> {days} day(s), {hours} hour(s)\")\n",
        "        if elapsed > checkin_timer:\n",
        "          checkin_flag = True\n",
        "          return elapsed, days, hours, checkin_flag\n",
        "        return elapsed, days, hours, checkin_flag"
      ],
      "metadata": {
        "id": "zQL7Bd_M54Ac"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chatting(user_input, message_history):\n",
        "  \"\"\"Interface outlet for chatbot response.\"\"\"\n",
        "\n",
        "  state, chatbot_msg = chatbot(user_input, message_history, state)\n",
        "  #chatbot_msg = state[\"chat_display\"][-1][1]\n",
        "\n",
        "  \"\"\"message_history = [\n",
        "                  {\"role\": \"user\", \"content\": f\"{user_input}\"},\n",
        "                  {\"role\": \"assistant\", \"content\": \"Mental Wealth\"}\n",
        "                ]\n",
        "  if checkin_flag is False:\n",
        "    elapsed, days, hours, checkin_flag = duration_timer(user_input, checkin_timer, verbose)\n",
        "    return chatbot_msg\n",
        "  elif checkin_flag:\n",
        "    # checkin_msg = initiate_reg_checkin() Force bot to use @tool regular checkin\n",
        "    # note_take(elasped, days, hours, message_history[-1]) Record inactivity timestamp\"\"\"\n",
        "  return chatbot_msg\n",
        "\n",
        "\n",
        "with gr.Blocks() as extra:\n",
        "  cute=gr.Image(value=\"cute-fox.gif\", label=\"GIF\", show_label=True)\n",
        "  system_prompt = gr.Textbox(\"You are helpful AI.\", label=\"System Prompt\")\n",
        "  #help_resources_button = gr.Button(10, 100, render=False)\n",
        "\n",
        "import random\n",
        "\n",
        "def random_response(message, history):\n",
        "    return random.choice([\"Yes\", \"No\"])\n",
        "\n",
        "#client = Client(\"http://0.0.0.0:7860\")\n",
        "#result = client.predict()\n",
        "interface = gr.ChatInterface(\n",
        "    fn=chatting,\n",
        "    type=\"messages\",\n",
        "    save_history=True,\n",
        ")\n",
        "\"\"\"interface = gr.ChatInterface(\n",
        "    fn=chatting,\n",
        "    type=\"messages\",\n",
        "    chatbot=gr.Chatbot(height=300),\n",
        "    textbox=gr.Textbox(placeholder=\"Type here~! =)\", container=False, scale=7),\n",
        "    title=\"SHALA CHAT\",\n",
        "    theme=\"ocean\",\n",
        "    save_history=True,\n",
        "    )\"\"\"\n",
        "\n",
        "if realtime_test == True:\n",
        "  interface.launch(share=True)\n",
        "  # gr.load_chat(base_url=\"http://localhost:11434/v1/\", model=agent_model, token=\"250\", file_types=None).launch()\n",
        "  \"\"\"with gr.Blocks() as interface:\n",
        "    gr.Markdown(\"Shala Chat\")\n",
        "    gr.Image(height=250, )\n",
        "    with gr.Row(equal_height=True):\n",
        "      full_chat = gr.Textbox(lines=250, show_label=True)\n",
        "      chatbox = gr.Textbox(lines=1, show_label=False)\n",
        "      button = gr.Button(\"Submit\", varient=\"primary\")\n",
        "\n",
        "    button.click(\n",
        "          submit_msg = user_input,\n",
        "          inputs = chatbox,\n",
        "          # shala_response = new_output,\n",
        "      )\n",
        "\n",
        "  user_interface(user_name)\"\"\""
      ],
      "metadata": {
        "id": "hCg6PLAokJpq",
        "outputId": "8e409aa6-b720-4901-f68d-7907c4f5ed97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded as API: http://0.0.0.0:7860/ ✔\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "No value provided for required argument: message",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-71-9c8635455636>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"http://0.0.0.0:7860\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m interface = gr.ChatInterface(\n\u001b[1;32m     33\u001b[0m     \u001b[0mfn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchatting\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gradio_client/client.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, api_name, fn_index, *args, **kwargs)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[1;32m    475\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_infer_fn_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m         return self.submit(\n\u001b[0m\u001b[1;32m    477\u001b[0m             \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m         ).result()\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gradio_client/client.py\u001b[0m in \u001b[0;36msubmit\u001b[0;34m(self, api_name, fn_index, result_callbacks, *args, **kwargs)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEndpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstruct_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m         \u001b[0mhelper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gradio_client/utils.py\u001b[0m in \u001b[0;36mconstruct_args\u001b[0;34m(parameters_info, args, kwargs)\u001b[0m\n\u001b[1;32m   1276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_Keywords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNO_VALUE\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m         raise TypeError(\n\u001b[0m\u001b[1;32m   1279\u001b[0m             \u001b[0;34mf\"No value provided for required argument: {kwarg_names[_args.index(_Keywords.NO_VALUE)]}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m         )\n",
            "\u001b[0;31mTypeError\u001b[0m: No value provided for required argument: message"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"def user_interface(user_name:str=\"Friend\", intensity:int=0, ):\n",
        "    return \"Hello, \" + user_name + \"!\" * int(intensity)\n",
        "\n",
        "interface = gr.Interface(\n",
        "    fn=greet,\n",
        "    inputs=[\"text\", \"slider\"],\n",
        "    outputs=[\"text\"],\n",
        ")\n",
        "\n",
        "interface.launch()\"\"\""
      ],
      "metadata": {
        "id": "S5XZqezxq6qr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBQN5Bpga0I1"
      },
      "source": [
        "Visual Workflow of nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nwgo5oQRa5_I"
      },
      "outputs": [],
      "source": [
        "Image(chat_with_human_graph.get_graph().draw_mermaid_png()) # Current node mermaid diagram"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}