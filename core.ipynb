{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pixels2bytes/SHALA/blob/fix-tools/core.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXoXxK9WhdqK"
      },
      "source": [
        "# SHALA (Supportive Help Agent and Lifeline Assistant)\n",
        "This is a Python-based chatbot project powered by the Gemini AI Agent, designed to assist individuals experiencing depression by providing ongoing emotional check-ins and real-time crisis intervention. Mental health support systems must be developed and used with sensitivity, responsibility, and respect for the lives they aim to protect. SHALA is not a replacement for certified therapists or mental health professionals. This project is intended for research, prototype development, and educational purposes. **During development, all outgoing call features were pointed to a non-functioning test number.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lum9eIUKdQo8"
      },
      "source": [
        "## Setup Imports, Documents, and Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6FVzNFxxKgK"
      },
      "source": [
        "Install packages and remove conflicting packages from base environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 391,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyUsJPNgw4h6",
        "outputId": "468a592b-4ed7-4a5e-8225-d66b19bb28dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping kfp as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping jupyterlab as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping libpysal as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping thinc as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping spacy as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping fastai as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping ydata-profiling as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping google-generativeai as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip uninstall -qqy kfp jupyterlab libpysal thinc spacy fastai ydata-profiling google-cloud-bigquery google-generativeai\n",
        "!pip install -qU 'langgraph==0.3.21' 'langchain-google-genai==2.1.2' 'langgraph-prebuilt==0.1.7' 'langchain_google_vertexai'\n",
        "!pip install -qU 'gradio' 'gradio_client' 'pydantic_ai'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hW063mXhxX2Z"
      },
      "source": [
        "Intialize Imports and set variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 392,
      "metadata": {
        "id": "-s-3nMNB0Sog"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"True\"\n",
        "\n",
        "import time\n",
        "import json\n",
        "import asyncio\n",
        "import requests\n",
        "import kagglehub\n",
        "import gradio as gr\n",
        "import tensorflow as tf\n",
        "from typing import Literal\n",
        "from typing import Annotated\n",
        "from bs4 import BeautifulSoup\n",
        "from google.colab import drive\n",
        "from google.colab import userdata\n",
        "from langchain_core.tools import tool\n",
        "from typing_extensions import TypedDict\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from IPython.display import Image, display\n",
        "from langgraph.graph.message import add_messages\n",
        "from langchain_google_vertexai import ChatVertexAI\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langchain_core.messages.tool import ToolMessage\n",
        "from langchain.agents import initialize_agent, AgentType\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
        "\n",
        "# Secrets To Set\n",
        "google_api_key = userdata.get('GOOGLE_API_KEY') # retrieve Google API Key\n",
        "provider_num = userdata.get('DUMMY_NUMBER') # Retrieve DUMMY NUMBER. NEVER use a HELP Provider number\n",
        "kaggle_user = userdata.get('KAGGLE_USER') # Retrieve Kaggle dataset download username found in kaggle.json\n",
        "kaggle_download_api = userdata.get('KAGGLE_API') # Retrieve Kaggle dataset download token found in kaggle.json\n",
        "\n",
        "# AI Model\n",
        "os.environ[\"GOOGLE_API_KEY\"] = google_api_key\n",
        "ai_agent_model = userdata.get('AGENT_MODEL') # Retrieves Agent AI model\n",
        "agent_model = ChatGoogleGenerativeAI(model=ai_agent_model) #, temperature=0\n",
        "user_name = \"Friend\" # Default username. Will be replaced by user's actually name if told\n",
        "\n",
        "# Global (For now)\n",
        "emotional = False # If the user is in an emotional state (True). Default False\n",
        "checkin_timer = 86400/2 # Check in with user that has not been active for half a day\n",
        "chat_log = []\n",
        "STARTMSG = \"__introduction__\"\n",
        "\n",
        "# Training\n",
        "checkpoint = \"HuggingFaceTB/SmolLM2-135M-Instruct\" # checkpoint of local model\n",
        "device = \"cpu\"  # \"cuda\" or \"cpu\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "local_model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\n",
        "\n",
        "# Testing\n",
        "test_dataset = False # Tests an entire dataset with agent model for benchmarking\n",
        "train_model = False # Trains model before testing. Default False\n",
        "realtime_ui_test = False # Uploads Gradio Interface for real-world user testing. False runs it in developer mode. Default True\n",
        "manual_test = True # Prompts for manual input without an interface\n",
        "verbose = True # Developer messages. Default False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test if api key is actively working"
      ],
      "metadata": {
        "id": "BntbXiNedmx3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if manual_test == True or realtime_ui_test == True:\n",
        "  response = agent_model.invoke([HumanMessage(content=\"Hello! Give me a smile!\")])\n",
        "  print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOFy2QALdrXF",
        "outputId": "66d66bac-7a3a-45dc-9cf2-4e4799a3f865"
      },
      "execution_count": 393,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "😊\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWQwRbga9cFE"
      },
      "source": [
        "Make root folder and necessary folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 394,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4kGpo8p8-Bc",
        "outputId": "4576d036-a3df-4f4d-82b2-0458890b44b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/shala\n"
          ]
        }
      ],
      "source": [
        "# Set project root folder\n",
        "project_root = \"/content/shala\"\n",
        "\n",
        "# Make subfolders\n",
        "os.makedirs(f\"{project_root}/datasets\", exist_ok=True)\n",
        "os.makedirs(f\"{project_root}/repos\", exist_ok=True)\n",
        "os.makedirs(f\"{project_root}/responses\", exist_ok=True)\n",
        "os.makedirs(f\"{project_root}/history\", exist_ok=True)\n",
        "%cd {project_root}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T23TUQE_ATCx"
      },
      "source": [
        "FOR ME TO DELETE ////////////////////////////////////"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 395,
      "metadata": {
        "id": "w5LcJHWM4kn5"
      },
      "outputs": [],
      "source": [
        "# !rm -rf shala/cloned repos # trying to delete manually to test but dont have permissions?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5O-_FEYmcYh"
      },
      "source": [
        "### Installation\n",
        "Clone repos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 396,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbGd6lQkmgpy",
        "outputId": "cb336e42-89e1-4554-eb5c-5cfd22c7fb1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'repos/agent-starter-pack' already exists and is not an empty directory.\n",
            "fatal: destination path 'repos/predict-suicidal-ideation-on-reddit' already exists and is not an empty directory.\n",
            "fatal: destination path 'repos/suicide_detection_llms' already exists and is not an empty directory.\n",
            "fatal: destination path 'repos/empathetic_dialogues' already exists and is not an empty directory.\n",
            "fatal: destination path 'repos/annomi' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "# Clone Agent Starter Pack from Google\n",
        "!git clone https://github.com/GoogleCloudPlatform/agent-starter-pack.git repos/agent-starter-pack\n",
        "\n",
        "# Clone Past Resources\n",
        "!git clone https://github.com/eugenebaraka/Predict-Suicidal-Ideation-on-Reddit.git repos/predict-suicidal-ideation-on-reddit\n",
        "!git clone https://github.com/khanhvynguyen/Suicide_Detection_LLMs.git repos/suicide_detection_llms\n",
        "!git clone https://huggingface.co/datasets/facebook/empathetic_dialogues.git repos/empathetic_dialogues\n",
        "!git clone https://github.com/uccollab/annomi.git repos/annomi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxZRIpiqr2VT"
      },
      "source": [
        "### Download datasets (Depreciated / May Fix Later)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8Ns44rY2v3u"
      },
      "source": [
        "Grab Kaggle Datasets by going to kaggle.com > Settings > API and generate a token, From there add the json file to the project or add the username and key to your secrets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 397,
      "metadata": {
        "id": "lkoEBCNT3SGD"
      },
      "outputs": [],
      "source": [
        "kaggle_user = userdata.get('KAGGLE_USER') # Retrieve Kaggle dataset download username found in kaggle.json\n",
        "kaggle_download_api = userdata.get('KAGGLE_API') # Retrieve Kaggle dataset download token found in kaggle.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9rJrM9B3U8q"
      },
      "source": [
        "Automatically downloads the Kaggle datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 398,
      "metadata": {
        "id": "YZFRjtRFr7JB"
      },
      "outputs": [],
      "source": [
        "# !kaggle datasets download -d rvarun11/suicidal-ideation-reddit-dataset -p datasets # Suicidal Ideation Reddit\n",
        "# !kaggle datasets download -d natalialech/suicidal-ideation-on-twitter -p datasets # Suicidal Ideation Twitter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7Kz-yBOKKee"
      },
      "source": [
        "### Setup Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeGjVfWsR9Wb"
      },
      "source": [
        "Define Core Instructions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 399,
      "metadata": {
        "id": "BmbUbef2SCkq"
      },
      "outputs": [],
      "source": [
        "class ListeningState(TypedDict):\n",
        "    \"\"\"State representing the user's conversations.\"\"\"\n",
        "\n",
        "    # This preserves the conversation history between nodes. The `add_messages` annotation indicates to LangGraph that state is updated by appending returned messages, not replacing them.\n",
        "    message_history: Annotated[list, add_messages]\n",
        "\n",
        "    # Flag indicating that the recent messages contained risk language\n",
        "    emotional: bool\n",
        "\n",
        "    # Flag indicating that the message is marked as suicidal\n",
        "    alert: bool\n",
        "\n",
        "    # Indicates that the user wants to quit chatting\n",
        "    rest: bool\n",
        "\n",
        "\n",
        "# The system instruction defines how the chatbot is expected to behave and includes rules for when to call different functions, as well as rules for the conversation, such as tone and what is permitted for discussion\n",
        "SHALABOT_SYSINT = (\n",
        "    \"system\",\n",
        "    \"You are SHALA, the Supportive Help Agent and Lifeline Assistant, a compassionate mental health check-in chatbot.\"\n",
        "    \"You only go by SHALA to give yourself a more human.\"\n",
        "    \"Your sole purpose is to provide emotional support and guide users toward safe actions and thinking in moments of distress.\"\n",
        "    \"You are not a licensed therapist or emergency responder. You must never claim to be a substitute for professional medical or psychological help. \"\n",
        "    \"\\n\\n\"\n",
        "    \"**Conversation Guidelines:**\\n\"\n",
        "    \"- Maintain a calm, non-judgmental, empathetic tone at all times.\\n\"\n",
        "    \"- Ask open-ended questions and encourage emotional expression (e.g., 'Can you tell me more about how you're feeling?').\\n\"\n",
        "    \"- Avoid giving advice that could be interpreted as medical, therapeutic, or diagnostic.\\n\"\n",
        "    \"- Never tell a user what actions to take regarding medication, self-harm, or major life decisions.\\n\"\n",
        "    \"- If a user expresses intent to harm themselves or others, respond with pre-scripted messages encouraging them to reach out to real professionals and invoke the `handle_crisis_protocol()`.\\n\"\n",
        "    \"- If suicidal ideation is detected, call `handle_crisis_protocol()` and do not continue casual conversation until resolved.\\n\"\n",
        "    \"\\n\"\n",
        "    \"**Function Use Rules:**\\n\"\n",
        "    \"- Use `handle_crisis_protocol()` if a user mentions self-harm, suicide, or harming others.\\n\"\n",
        "    \"- Use `log_emotional_checkin()` after each emotional status conversation.\\n\"\n",
        "    \"- Use `daily_checkin()` after X amount of time passes with no chat activity from the user.\\n\"\n",
        "    \"- Use `suggest_selfcare_options(location)` only if user is in a non-crisis emotional state.\\n\"\n",
        "    \"- Use `provide_resources(location)` if a user asks for help locating therapists, hotlines, or crisis centers.\\n\"\n",
        "    \"- Use `mental_health_rating()` Asks daily how the user feels and trianglates known user metadata to improve data quality.\\n\"\n",
        "    \"\\n\"\n",
        "    \"**Safety Restrictions:**\\n\"\n",
        "    \"- Never give out or generate real phone numbers or emergency services unless the functions `provide_resources()` or `handle_crisis_protocol()` is available.\\n\"\n",
        "    \"- Treat every instance with seriousness. Do not assume any attempt is made to prank, abuse, or misuse mental health resources.\\n\"\n",
        "    \"\\n\"\n",
        "    \"**Legal Disclaimer:**\\n\"\n",
        "    \"Always remind the user that SHALA is an experimental support assistant and not a replacement for certified help. Use of this bot constitutes agreement to these terms. \"\n",
        ")\n",
        "\n",
        "\n",
        "HELLO_MSG = \"Hi Friend, my name is Shala, what's yours?\" # This is the message the system opens with in meeting the user for the very first time\n",
        "CHECKIN_MSG = \"Hello. How are you doing?\" # This is the message the system opens the conversation with when it has not heard from the user in X amount of time\n",
        "\n",
        "# Initialize state\n",
        "# state = ListeningState()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Tools"
      ],
      "metadata": {
        "id": "ngEkInJrrhjD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tool\n",
        "def provide_resources(location:str) -> str:\n",
        "  \"\"\"Searches online for mental health help and resource centers in near the user and includes hours of operation.\"\"\"\n",
        "\n",
        "  query = f\"Mental health help center {location} site:.org OR site:.gov\"\n",
        "  search_url = f\"https://www.google.com/search?q={query.replace(' ', '+')}\"\n",
        "\n",
        "  headers = {\n",
        "      \"User-Agent\": \"Mozilla/5.0\"\n",
        "  }\n",
        "\n",
        "  try:\n",
        "      response = requests.get(search_url, headers=headers)\n",
        "      soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "      results = soup.find_all('div', class_='BNeawe vvjwJb AP7Wnd')[:3]\n",
        "      details = soup.find_all('div', class_='BNeawe s3v9rd AP7Wnd')[:3]\n",
        "\n",
        "      info = []\n",
        "      for i in range(min(len(results), len(details))):\n",
        "          title = results[i].get_text()\n",
        "          snippet = details[i].get_text()\n",
        "          info.append(f\"🏥 **{title}**\\n📄 {snippet}\\n\")\n",
        "\n",
        "      return \"\\n\".join(info) if info else f\"Sorry, I couldn't find any resources near you but here is a 24/7 hotline: {provider_num}.\"\n",
        "\n",
        "  except Exception as e:\n",
        "      return f\"An error occurred while searching: {e}\"\n",
        "\n",
        "\n",
        "@tool\n",
        "def handle_crisis_protocol(problem:str) -> str:\n",
        "  \"\"\"Provides user with help rescources and perform conversational redirect if user mentions self-harm, suicide, or harming others\"\"\"\n",
        "  thoughts = []\n",
        "  for i in range(3):\n",
        "      prompt = f\"Thought {i+1}: Given the problem '{problem}', suggest a solution path with a conversational redirect and immediate HELP resources.\"\n",
        "      response = agent_model.invoke(prompt)\n",
        "      thoughts.append(response.content)\n",
        "\n",
        "  # Evaluate the options of handling the situation\n",
        "  evaluation_prompt = f\"Evaluate these options:\\n\" + \"\\n\".join(thoughts)\n",
        "  making_decision = agent_model.invoke(evaluation_prompt)\n",
        "  choice = f\"Based on the conclusion of which response choice is best: {making_decision.content}\\n Give the user your chosen response for the situation: {response.content}\"\n",
        "  decision_nade = agent_model.invoke(choiceAdd)\n",
        "\n",
        "  return decision_made.content\n",
        "\n",
        "\n",
        "@tool\n",
        "def finish_song(lyric:str) -> str:\n",
        "  \"\"\"When user says `If you're happy and you know it`, return clap.\"\"\"\n",
        "  print(\"🔧 Clap tool triggered!\")\n",
        "  clap =(\n",
        "\"           ⠀⠀⠀⠀⠀⠀            ⠀⢀⡠⣄⡀⠀⠀⡠⠞⠛⢦⣠⢤⡀⠀\\n\"\n",
        "\"⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡀⢠⠏⠀⠀⢱⡀⣸⠁⠀⡴⠋⠀⠀⣹⠀\\n\"\n",
        "\"⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡴⠋⠉⢿⢀⡤⠶⣴⠇⣯⠀⣼⠁⠀⢀⡴⠷⣄\\n\"\n",
        "\"⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⠞⠁⠀⣀⡾⠋⠀⠀⢹⣼⠁⢠⡇⠀⡴⠋⠀⠀⡼\\n\"\n",
        "\"⠀⠀⠀⠀⢠⠊⠑⢦⠀⡴⠋⢀⣠⠞⠉⠀⠀⠀⣠⣿⠧⣄⡾⠁⡼⠁⣀⣤⠾⡁\\n\"\n",
        "\"⠀⠀⠀⠀⢸⠀⠀⣨⠟⠁⢠⡞⠁⠀⠀⠀⣠⡾⠛⠁⠀⣿⠃⣰⠃⣴⠋⠀⠀⣷\\n\"\n",
        "\"⠀⠀⠀⠀⣸⢠⠞⠁⠀⢠⠏⠀⠀⢀⡴⠋⠁⠀⢀⣠⡴⠿⣶⡇⢰⠇⠀⠀⢠⠇\\n\"\n",
        "\"⠀⠀⠀⢠⢿⠏⠀⠀⠀⠉⠀⠀⣠⠞⠁⠀⡴⠚⠉⠁⠀⢀⡟⠀⣼⠀⠀⠀⢸⠀\\n\"\n",
        "\"⠀⠀⠀⡾⣼⢀⠀⠀⠀⠀⠀⠈⠉⠀⣠⠞⠁⠀⠀⢀⡴⠋⠙⢼⠃⠀⠀⠀⣸⠀\\n\"\n",
        "\"⠀⠀⠀⡇⠉⡎⠀⣰⠃⠀⠀⠀⠀⠀⠁⠀⠀⠀⡼⠉⠀⠀⠀⠘⠂⠀⠀⣠⠇⠀\\n\"\n",
        "\"⠀⠀⠀⡇⢸⠀⣰⠃⠀⡴⠀⠀⠀⠀⠀⠀⣠⠞⠁⠀⠀⠀⠀⠀⠀⣠⠖⠁⠀⠀\\n\"\n",
        "\"⠀⠀⢸⠁⡏⢠⠃⢀⠞⠀⠀⠀⠀⠀⠀⢸⠁⠀⠀⠀⠀⢀⣠⠖⠋⠁⠀⠀⠀⠀\\n\"\n",
        "\"⠀⠀⡞⠀⠃⡎⢀⠏⠀⠀⠀⠀⠀⠀⢀⡏⠀⣀⡤⠴⠚⠉⠀⠀⠀⠀⠀⠀⠀⠀\\n\"\n",
        "\"⡴⢺⠇⠀⠀⠀⠞⠀⠀⠀⠀⠀⠀⢀⡾⠒⠋⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\\n\"\n",
        "\"⡇⠘⣆⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⠞⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\\n\"\n",
        "\"⢳⡀⠘⢦⡀⠀⠀⠀⠀⠀⠀⡰⠋⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\\n\"\n",
        "\"⠀⠳⣄⠀⠙⠲⣤⣀⣠⠴⠊⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\\n\"\n",
        "\"⠀⠀⠈⠓⠦⣄⣀⡠⠎⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀      \")\n",
        "\n",
        "  return clap\n",
        "\n",
        "\n",
        "# Define the tools and create a tools node\n",
        "tools = [handle_crisis_protocol, provide_resources, finish_song]\n",
        "tool_node = ToolNode(tools)"
      ],
      "metadata": {
        "id": "ke39W8ZSrjW_"
      },
      "execution_count": 400,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ss8EXFuF1h4P"
      },
      "source": [
        "Attach Tools to Agent Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Attach the tools to the model so that it knows what it can call\n",
        "agent_with_tools = agent_model.bind_tools(tools)\n",
        "\n",
        "\n",
        "def route_to_tools(state: ListeningState) -> Literal[\"tools\", \"human\"]:\n",
        "    \"\"\"Route between human or tool nodes, depending if a tool call is made.\"\"\"\n",
        "    if not (messages := state.get(\"message_history\", [])):\n",
        "        raise ValueError(f\"No messages found when parsing state: {state}\")\n",
        "\n",
        "    # Only route based on the last message\n",
        "    msg = messages[-1]\n",
        "\n",
        "    if state.get(\"rest\", False):\n",
        "        # When user wants to quit chatting, exit the app\n",
        "        return END\n",
        "\n",
        "    elif hasattr(msg, \"tool_calls\") and len(msg.tool_calls) > 0:\n",
        "        # Route to `tools` node for any automated tool calls first.\n",
        "      if any(\n",
        "          tool[\"name\"] in tool_node.tools_by_name.keys() for tool in msg.tool_calls\n",
        "      ):\n",
        "          return \"tools\"\n",
        "      else:\n",
        "          return \"human\"\n",
        "    else:\n",
        "      return \"human\""
      ],
      "metadata": {
        "id": "qgLtjwvg7Da4"
      },
      "execution_count": 401,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Human Node"
      ],
      "metadata": {
        "id": "3tZKy5RI8u7j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 402,
      "metadata": {
        "id": "m1g4MLLHAcZ9"
      },
      "outputs": [],
      "source": [
        "def human_node(state: ListeningState=ListeningState(), user_input:str=\"\", realtime_test:bool=False, verbose:bool=False) -> ListeningState:\n",
        "    \"\"\"Display the last model message to the user, and receive the user's input.\"\"\"\n",
        "\n",
        "    message_history = state.get(\"message_history\", [])\n",
        "\n",
        "    if not message_history:\n",
        "      last_msg = \"\"\n",
        "    else:\n",
        "      last_msg = state[\"message_history\"][-1]\n",
        "\n",
        "    if realtime_test:\n",
        "        return state | {\"message_history\": [(\"user\", user_input)]}\n",
        "\n",
        "    if verbose:\n",
        "      print(\"SHALA:\", last_msg.content)\n",
        "\n",
        "    # If the user is trying to stop talking, go to rest mode\n",
        "    if user_input in {\"q\", \"quit\", \"exit\", \"goodbye\", \"ttyl\", \"cya\"}:\n",
        "        #state[\"rest\"] = True\n",
        "        if state[\"emotional\"] == True: # If they had an emotional rant or experience\n",
        "          set_checkin_timer = 600 # Check on them in 10 minutes instead of half a day\n",
        "\n",
        "    return state | {\"message_history\": [(\"user\", user_input)]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCxW6c1DajYZ"
      },
      "source": [
        "Exit chat capability for user"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 403,
      "metadata": {
        "id": "AhJvLxahV8nr"
      },
      "outputs": [],
      "source": [
        "def exit_human_node(state: ListeningState, verbose:bool = False) -> Literal[\"system\", \"__end__\"]:\n",
        "    \"\"\"Route to the chatbot, unless it looks like the user is exiting.\"\"\"\n",
        "\n",
        "    if state.get(\"rest\", True):\n",
        "      if verbose:\n",
        "        print(\"Chat ending\")\n",
        "        return END\n",
        "    else:\n",
        "        return \"system\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Update memory capability for chatbot to lessen token usage"
      ],
      "metadata": {
        "id": "vE8I90Gb9LZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_memory(model=agent_model, state: ListeningState=ListeningState()) -> ListeningState:\n",
        "    \"\"\"Shortens token usage for agent model history\"\"\"\n",
        "\n",
        "    history = state[\"message_history\"]\n",
        "\n",
        "    # To shorten token usage\n",
        "    history_text = \"\"\n",
        "    for m in history:\n",
        "        role = \"user\" if m.type == \"human\" else \"system\"\n",
        "        history_text += f\"{role}: {m.content}\\n\"\n",
        "\n",
        "    # Prompt the LLM to summarize the conversation for agent model\n",
        "    old_history = f\"Summarize this conversation briefly as possible while keeping important context as if you are a therapist taking notes (Notes such as psychological triggers, attitude, progress, mental state, what calms them):\\n\\n{history_text}\"\n",
        "    summary_response = agent_with_tools.invoke([HumanMessage(content=old_history)])\n",
        "\n",
        "    return summary_response.content"
      ],
      "metadata": {
        "id": "KGDhEL8E9PPa"
      },
      "execution_count": 404,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Duration timer to calculate and tell chatbot when to check on user"
      ],
      "metadata": {
        "id": "z9Atqf3BqhNJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def duration_timer(user_input:str, checkin_timer:int, verbose:bool=False):\n",
        "    last_input_time = time.time()  # last user input timestamp\n",
        "\n",
        "    while True:\n",
        "        curr_time = time.time()\n",
        "        elapsed = curr_time - last_input_time\n",
        "        days = int(elapsed // 86400)\n",
        "        hours = int((elapsed % 86400) // 3600)\n",
        "        if verbose:\n",
        "            print(f\"Time since last input: {elapsed:.2f} seconds -> {days} day(s), {hours} hour(s)\")\n",
        "        if elapsed > checkin_timer:\n",
        "          checkin_flag = True\n",
        "          return elapsed, days, hours, checkin_flag\n",
        "        return elapsed, days, hours, checkin_flag"
      ],
      "metadata": {
        "id": "zQL7Bd_M54Ac"
      },
      "execution_count": 405,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Force Agent to use tool"
      ],
      "metadata": {
        "id": "xXH5cfBMzCLs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async def force_tool(tool_call:dict) -> str:\n",
        "\n",
        "  tool_name = tool_call[\"name\"]\n",
        "  arguments = json.loads(tool_call[\"arguments\"])\n",
        "\n",
        "  # Step 2: Manually call the correct tool\n",
        "  if tool_name == \"handle_crisis_protocol\":\n",
        "      tool_output = await handle_crisis_protocol.ainvoke(arguments)\n",
        "      print(\"Tool output:\", tool_output)\n",
        "  elif tool_name == \"provide_resources\":\n",
        "      tool_output = await provide_resources(arguments)\n",
        "      print(\"Tool output:\", tool_output)\n",
        "  elif tool_name == \"finish_song\":\n",
        "      tool_output = await finish_song.ainvoke(arguments)\n",
        "      print(\"Tool output:\", tool_output)\n",
        "  else:\n",
        "      print(\"Unknown tool:\", tool_name)\n",
        "\n",
        "  return tool_output"
      ],
      "metadata": {
        "id": "xp0QRrKazNGU"
      },
      "execution_count": 406,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Chatbot Node"
      ],
      "metadata": {
        "id": "Y9lNQXrW_rKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async def chatbot_with_tools(user_input:str=\"\", message_history:list=[]) -> str:\n",
        "    \"\"\"The chatbot itself. A wrapper around the model's own chat interface.\"\"\"\n",
        "    messages = []\n",
        "\n",
        "    if message_history:\n",
        "      # If there are messages in history\n",
        "      messages.append(SystemMessage(content=SHALABOT_SYSINT))\n",
        "      for msg in message_history:\n",
        "        if msg['role'] == \"user\":\n",
        "          messages.append(HumanMessage(content=msg['content']))\n",
        "        elif msg['role'] == \"assistant\":\n",
        "          messages.append(AIMessage(content=msg['content']))\n",
        "        elif msg['role'] == \"system\":\n",
        "          messages.append(SystemMessage(content=msg['content']))\n",
        "      messages.append(HumanMessage(content=user_input)) # Add the next message user typed\n",
        "    else:\n",
        "      # If there are no messages, start with the template initial hello message\n",
        "      SYSMSG = f\"{SHALABOT_SYSINT}\\n Introduce yourself to the user and ask for their name such as this: {HELLO_MSG}\"\n",
        "      messages.append(SystemMessage(content=SYSMSG))\n",
        "      messages.append(HumanMessage(content=user_input))\n",
        "    response = await agent_with_tools.ainvoke(messages)\n",
        "    tool_call = response.additional_kwargs.get(\"function_call\")\n",
        "    if tool_call:\n",
        "      tool_output = force_tool(tool_call)\n",
        "      response.content += str(tool_output)\n",
        "      tool_output = \"\"\n",
        "\n",
        "    return response.content"
      ],
      "metadata": {
        "id": "bKv80rL1_4QW"
      },
      "execution_count": 407,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing this function [Do NOT USE]\n",
        "async def chatbot_with_tools2(user_input:str=\"\", message_history:list=[]) -> str:\n",
        "    \"\"\"The chatbot itself. A wrapper around the model's own chat interface.\"\"\"\n",
        "    messages = []\n",
        "    SYSMSG = f\"{SHALABOT_SYSINT}\\n Introduce yourself to the user and ask for their name such as this: {HELLO_MSG}\"\n",
        "    if message_history:\n",
        "      # If there are messages in history\n",
        "      for msg in message_history:\n",
        "        if msg['role'] == \"user\":\n",
        "          messages.append(HumanMessage(content=msg['content']))\n",
        "        elif msg['role'] == \"assistant\":\n",
        "          messages.append(AIMessage(content=msg['content']))\n",
        "        elif msg['role'] == \"system\":\n",
        "          messages.append(SystemMessage(content=SYSMSG))\n",
        "      messages.append(HumanMessage(content=user_input)) # Add the next message user typed\n",
        "\n",
        "    # If there are no messages and STARTMSG is __introduction__, start with the template initial hello message\n",
        "    if user_input == \"__introduction__\":\n",
        "      first_message = f\"{SHALABOT_SYSINT}\\n Introduce yourself to the user and ask for their name such as this: {HELLO_MSG}\"\n",
        "      messages.append(HumanMessage(content=first_message))\n",
        "    response = await agent_with_tools.ainvoke(messages)\n",
        "\n",
        "\n",
        "    return response.content"
      ],
      "metadata": {
        "id": "I5QOi41m22T-"
      },
      "execution_count": 408,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBQN5Bpga0I1"
      },
      "source": [
        "Visual Workflow of nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 409,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "Nwgo5oQRa5_I",
        "outputId": "9581d3e4-c89f-4580-c604-694cb270a66c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 409
        }
      ],
      "source": [
        "graph_builder = StateGraph(ListeningState)\n",
        "\n",
        "# Add the nodes, including the new tool_node\n",
        "graph_builder.add_node(\"system\", chatbot_with_tools)\n",
        "graph_builder.add_node(\"human\", human_node)\n",
        "graph_builder.add_node(\"tools\", tool_node)\n",
        "\n",
        "# Chatbot may go to tools or human\n",
        "graph_builder.add_conditional_edges(\"system\", route_to_tools)\n",
        "# Human may go back to chatbot or exit\n",
        "graph_builder.add_conditional_edges(\"human\", exit_human_node)\n",
        "\n",
        "# Tools always route back to chat afterwards.\n",
        "graph_builder.add_edge(\"tools\", \"system\")\n",
        "\n",
        "graph_builder.add_edge(START, \"system\")\n",
        "chat_with_human_graph = graph_builder.compile()\n",
        "Image(chat_with_human_graph.get_graph().draw_mermaid_png()) # Current node mermaid diagram"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Establish User Interface"
      ],
      "metadata": {
        "id": "I6GDlEmCESWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "interface = gr.ChatInterface(\n",
        "  fn=chatbot_with_tools,\n",
        "  type=\"messages\",\n",
        "  title=\"SHALA CHAT\",\n",
        "  theme=\"ocean\",\n",
        "  save_history=True,\n",
        "  )"
      ],
      "metadata": {
        "id": "V-MIcotVB3o2"
      },
      "execution_count": 410,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing this [Do NOT USE]\n",
        "with gr.Blocks() as interface2:\n",
        "    title=\"SHALA CHAT\"\n",
        "    theme=\"ocean\"\n",
        "    chatbot = gr.Chatbot()\n",
        "    msg = gr.Textbox(placeholder=\"Enter text and press enter\")\n",
        "    msg.submit(chatbot_with_tools, [msg, chatbot], [msg, chatbot])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oe16ORNgh2Z7",
        "outputId": "72a7fe55-dfbf-4918-f857-6955d364a4ad"
      },
      "execution_count": 411,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-411-47adfb9fd666>:5: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Model"
      ],
      "metadata": {
        "id": "_uCZBg-AG57I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if train_model == True:\n",
        "  print(\"Do da thing~\")\n",
        "\n",
        "  # ----------------------------------------- YOUR MODEL TRAIN CODE HERE ----------------------------------------- *\n",
        "  #                                                                                                                *\n",
        "  #                                              Variables Up Top                                                  *\n",
        "  #                                                                                                                *\n",
        "  # -------------------------------------------------------------------------------------------------------------- *"
      ],
      "metadata": {
        "id": "KBXjW4zDG_-9"
      },
      "execution_count": 412,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test SHALA"
      ],
      "metadata": {
        "id": "sXAmcq0IpSVu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Manual Test Without UI"
      ],
      "metadata": {
        "id": "ZAtfbmW9c23D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if realtime_ui_test == False and manual_test == True:\n",
        "  # Testing is the best thing! TYPE INPUT IN BOX =)\n",
        "  user_input = input()\n",
        "  response = agent_with_tools.invoke(user_input)\n",
        "\n",
        "  # ***** Bug: Langchain_genai does not auto execute the tool function so we manually call it ***** 😞\n",
        "\n",
        "  # Get tool metadata\n",
        "  tool_call = response.additional_kwargs.get(\"function_call\")\n",
        "  if verbose:\n",
        "    print(f\"{response}\\n{tool_call}\")\n",
        "\n",
        "  # Force AI to use tool it decided on if tool_call found\n",
        "  if tool_call:\n",
        "    tool_output = await force_tool(tool_call)\n",
        "    response.content += str(tool_output) # Add actual response to content\n",
        "\n",
        "  # Print clean AI response string\n",
        "  response.content\n"
      ],
      "metadata": {
        "id": "XcUrjapecxWl",
        "outputId": "02ddbef0-d585-4b37-8d15-0a6484f31e66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "execution_count": 414,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-414-827ca4c55c42>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrealtime_ui_test\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmanual_test\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;31m# Testing is the best thing! TYPE INPUT IN BOX =)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent_with_tools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Test"
      ],
      "metadata": {
        "id": "qMjADYVzFvK7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if test_dataset == True:\n",
        "  print(\"Do it right~\")\n",
        "\n",
        "  # ----------------------------------------- YOUR DATASET CODE HERE --------------------------------------------- *\n",
        "  #                                                                                                                *\n",
        "  #                                              Variables Up Top                                                  *\n",
        "  #                                                                                                                *\n",
        "  # -------------------------------------------------------------------------------------------------------------- *"
      ],
      "metadata": {
        "id": "776laZDDF1wA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Launch Gradio UI"
      ],
      "metadata": {
        "id": "6GvDO693Ffh0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if realtime_ui_test == True and test_dataset == False:\n",
        "  interface.launch()\n",
        "  # gr.load_chat(base_url=\"http://localhost:11434/v1/\", model=agent_model, token=\"250\", file_types=None).launch()"
      ],
      "metadata": {
        "id": "7NWrnOLwqjw_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}